<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://bastinflorian.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bastinflorian.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-03T09:15:04+00:00</updated><id>https://bastinflorian.github.io/feed.xml</id><title type="html">blank</title><subtitle>Graduated from a mathematics and informatics bachelor, I did a research master in AI at the most prestigious French school, ENS. After an internship as a NLP Data Scientist at Lili.ai, I joined Octo Technology, part of Accenture. I worked for several clients such as Le Monde, Casino, Club Med, Chanel and Pernod Ricard. I took different positions for these clients, Tech Lead Data, Data Scientist or Engineer, Deep Learning or LLM engineer. During these experiences, I developed a Machine and Deep Learning expertise on differents fields (pose estimation, gesture recognition, churn and propensity to subscribe modeling, Gen AI RAG chatbots). I also build data architectures and robust data pipelines (see our Apache Beam 2023 talk in New York). I improved my skills in the GCP and Azure Cloud working on it with my clients or with certifications. I discovered CI/CD and clean code &amp; architecture good practises. I recently got awarded by Google at their Gen AI competition in the category Superior GenAI Business Value and ROI. I am also a Generative AI &amp; GCP Cloud teacher at Tunis Dauphine University </subtitle><entry><title type="html">Comment utiliser le mutlimodal pour améliorer un chatbot RAG ? - OCTO Talks !</title><link href="https://bastinflorian.github.io/blog/2024/comment-utiliser-le-mutlimodal-pour-amliorer-un-chatbot-rag-octo-talks/" rel="alternate" type="text/html" title="Comment utiliser le mutlimodal pour améliorer un chatbot RAG ? - OCTO Talks !"/><published>2024-02-29T00:00:00+00:00</published><updated>2024-02-29T00:00:00+00:00</updated><id>https://bastinflorian.github.io/blog/2024/comment-utiliser-le-mutlimodal-pour-amliorer-un-chatbot-rag----octo-talks-</id><content type="html" xml:base="https://bastinflorian.github.io/blog/2024/comment-utiliser-le-mutlimodal-pour-amliorer-un-chatbot-rag-octo-talks/"><![CDATA[<table> <tbody> <tr> <td>“J’ai un histogramme comparatif dans mon PDF, pourquoi mon chatbot n’est-il pas capable de s’en servir pour me répondre ?”Générer le résumé d’un graphe, extraire de manière structurée un tableau, identifier les images pertinentes dans les documents, venez découvrir comment booster votre RAG grâce à l’usage raisonné des modèles multimodaux.Les approches RAG (Retrieval Augmented Generation) figurent parmi les plus utilisées pour l’exploitation des moteurs de langages. Les “Generative AI powered Chatbot” exploitent l’architecture RAG et simplifient la recherche documentaire des employé.e.s des entreprises avec lesquelles nous travaillons.Les produits qui en découlent sont nombreux : assistants RH &amp; IT augmentés, assistants business analystes pour la synthèse ou la comparaison de rapports, détecteurs de documentation absente ou obsolète…Cependant, ces assistants n’étaient pas en mesure de traiter et d’interpréter des images et schémas complexes dans leur base documentaire… jusqu’à l’arrivée du Multimodal avec GPT 4 Vision, LLava ou Gemini dont la version 1.5 est sortie le 15 février.Alors comment détecter, choisir et résumer des graphiques et tableaux dans vos sources documentaires en utilisant le multimodal ?C’est ce que nous allons résoudre au cours de cet article. Nous supposerons dans un premier temps que les images sont déjà localisées et extraites des documents. Nous verrons ensuite comment les détecter en utilisant les librairies existantes. Nous proposerons enfin de réduire l’usage coûteux (en euros, CO2 et secondes) du multimodal au strict nécessaire. Cet article s’adresse aux lecteurs ayant déjà une compréhension de l’architecture d’un RAG.Un repository de code est joint à cet article. Il contient l’intégralité des fonctions et analyses présentées et des expérimentations supplémentaires pour vous aider à intégrer le multimodal et ainsi améliorer la qualité de votre base vecteur.Les modèles multimodaux sont des modèles neuronaux ayant la capacité d’ingérer différentes modalités de données d’entrées. L’image, un ensemble de pixels et le texte peuvent être encodés dans un même espace vectoriel. GPT 4 et Gemini intègrent un text encoder, mais aussi vision encoder leur permettant de traiter les images et le texte. C’est en ce sens qu’il nous est possible de résumer et restructurer des images contenant des graphiques et d’y incorporer un prompt pour guider le modèle vers la sortie souhaitée.L’accès à ces modèles a tout d’abord été limité à certains utilisateurs (private preview) avant de s’étendre à l’ensemble des utilisateurs (public preview). Leur disponibilité générale (GA) varie selon les fournisseurs de modèles de vision, mais leur imminence est évidente. Cette disponibilité permettra à tous leurs utilisateurs de s’en servir pour des projets en production.Quelques contraintes sont cependant non négligeables à l’heure actuelle. Le temps de réponse des modèles vision est long. Une requête API pour un modèle vision non turbo peut facilement atteindre 20 secondes. S’en servir en brute force sur une base documentaire de 10 000 pages est déconseillé, car trop long et coûteux. De plus, les disponibilités actuelles limitent très fortement la possibilité de parallélisme.Bien que ces contraintes soient amenées à évoluer avec l’ouverture progressive des modèles multimodaux, n’oublions pas une métrique qui devrait être au cœur de nos projets, l’empreinte carbone de vos modèles.Cet article s’articulera donc autour de l’usage raisonné des modèles multimodaux.Lançons-nous avec une problématique que vous pourriez rencontrer lors du traitement de votre documentation et son ingestion dans une base vecteur : la gestion des tableaux. Bien que certaines étapes sont nécessaires pour conserver la structure d’un tableau au détriment de son extraction texte, ce premier exemple ne requiert pas forcément l’utilisation d’un modèle multimodal.Prenons le tableau suivant :Figure 1 : Notre tableau exemple - Comparatif des coûts par tokens pour chaque modèleL’objectif est de l’intégrer dans notre base documentaire afin d’obtenir la réponse à la question suivante :Peux-tu trier les LLM des cloud Azure et GCP selon leurs coûts par milliers de tokens ?Insérons le tableau de la Figure 1 dans un fichier PDF (l’approche pour un PPTX est couverte dans le repository GitHub associé à l’article). Ce tableau sera sous deux formats, un format texte et un format image. Nous utilisons Unstructured pour collecter le texte depuis ce powerpoint.Unstructured possède plusieurs spécificités intéressantes :Pour rappel, deux formats du même tableau ont été insérés dans notre document initial. Le premier, sous format texte, a bien été détecté. Le second sous format image n’a pas été détecté. En passant sur PDF, l’OCR aurait récupéré le texte du tableau en perdant sa structure, rendant complexe son interprétation pour un moteur de langage. Une métadonnée text_as_html est disponible pour l’élément faisant référence au tableau (sous format texte) et contient le code HTML pour le reconstruire.Voici la synthèse des extractions réalisées selon ces différents cas de figure : Figure 2 : Les différentes extractions d’un tableau selon son type (tableau, image) pour un PDF avec UnstructuredAnalysons les réponses à notre question suite aux différentes extractions du tableau :On a supposé ici que notre retriever a retrouvé le chunk relatif à l’extraction du tableau, et que sa sémantique est proche de la question. En réalité, ce ne sera pas forcément le cas. Nous développons ce point dans la partie suivante.Astuce : Le chunk contenant notre tableau sous format HTML est de 408 tokens. Dans le cas du PowerPoint, en remplaçant le format HTML par un tableau dont les cellules sont séparées par des ‘</td> <td>’, on réduit le tableau à 217 tokens sans altérer la qualité de la réponse. Cette méthode Pandas est la plus adaptée pour transformer un tableau HTML en tableau dont les cellules sont séparées par des ‘pipe’. Pandas utilise LXML en premier lieu pour le parsing avant d’utiliser Beautiful Soup en cas d’échec.En conclusion :Vous trouverez dans ici et ici le code relatif à cette partie, avec un approfondissement sur le type de réponse selon le choix (HTML, texte) et le type de documents d’origine (PPTX, PDF) des tableaux. Voyons maintenant comment améliorer sa sémantique et optimiser notre retriever.Considérons que notre tableau est bien extrait et étudions sa sémantique et son embedding. En prenant le tableau suivant :Figure 3 : Tableau en format texte extrait par Unstructured au format HTMLAinsi que son format en délimitation “</td> <td>” :Figure 4 : Tableau en format texte extrait par Unstructured au format HTML et transformé tableau texte avec Pandas et une méthode spécifiqueCalculons l’embedding (text-embedding-ada-002) de ces deux représentations textuelles et la similarité avec la question :Peux-tu trier les LLM des clouds Azure et GCP selon leurs coûts par milliers de tokens ?Résumons en anglais les deux formats de tables extraites à l’aide de GPT 4 :Les résumés sont les suivants :Figure 5 : Résumé du tableau avec séparateur ‘</td> <td>’Figure 6 : Résumé du tableau extrait avec balises HTMLLes similarités de ces résumés avec la question sont les suivantes:Nous constatons une augmentation de 2 points sur la similarité entre la question et les contenus résumés des tableaux. L’ajout d’un résumé pour un tableau améliore donc la performance du retriever. Vous trouverez ici le code relatif à cette partie avec une analyse plus approfondie sur la similarité cross-language et un test sur une autre question.En conclusion :Figure 7 : Architecture RAG possible pour l’extraction de tableaux. Le résumé est ‘retrouvé’ par le retriever, mais le format html est donné en contexteLorsque le tableau est au format image, nous avons vu que l’extraction OCR faîte par la librairie Unstructured n’était pas convaincante. Dans cette partie, nous réaliserons une étude des différents outils existants sur le marché pour améliorer cette détection.Les modèles OCR sont généralement composés de deux parties :En 1989, Yann LeCun propose un premier modèle d’OCR en utilisant des réseaux convolutifs ainsi que des couches fully connected. Je vous invite à aller voir cette vidéo démonstrative et la description associée.Une trentaine d’années plus tard, les approches OCR évoluent. Le développement de la recherche pour le traitement de l’image avec notamment les Mask R-CNN améliorent considérablement la détection des zones de texte. Les modèles récurrents tels que le LSTM et plus récemment, les modèles transformers améliorent la partie de reconnaissance du texte.Les librairies existantes sont nombreuses et permettent de résoudre l’un des deux ou les deux modules constitutifs d’un modèle OCR:Certains clouds tels que Google Cloud avec Document AI ou AWS avec Textract proposent leur outil intégrable facilement pour votre cas d’usage IA. Document AI propose notamment la détection de tableau et fournit un format de sortie conservant la structure du tableau.Pour l’extraction de tableaux au format image, Tesseract (libraire open source) et Document AI (en service managé) sont les deux outils qui vous seront le plus utiles.Vous trouverez dans la Figure 8 l’extraction et la structure détectée par Document AI. Cet outil permet d’exploiter pleinement le contexte du tableau sans en perdre la structure.Figure 8 : Extraction du tableau au format image de la Figure 1 avec Document AI (GCP). L’image est tronquée, mais les colonnes sont toutes détectéesCe paragraphe clos les contournements possibles du multimodal. Nous vous avions parlé du multimodal en introduction, mais vous avions aussi prévenu que son utilisation est coûteuse et doit s’effectuer de manière raisonnée. Dans la partie suivante, nous verrons comment utiliser le multimodal pour l’exploitation des graphiques au format image.Dans l’immense majorité des cas, les approches OCR ne sont pas suffisantes pour interpréter un graphique. Extraire les valeurs numériques présentes sur les axes, les points ou les barres verticales d’un nuage de points ou histogramme n’est pas suffisant. Le multimodal devient alors incontournable.GPT 4 Vision, LLava et Gemini peuvent s’utiliser de différentes manières comme le montre la Figure 9.Tout d’abord, le multimodal s’avère intéressant pour l’ingestion des documents dans une base vecteur. Lorsque la représentation visuelle d’un type de document améliore sa compréhension, l’usage du multimodal a du sens au détriment de son extraction du texte. Dans le cas d’un tableau, la perte de la structure est impactante. En cas de valeurs manquantes ou d’une structure particulière, il devient impossible pour un moteur de langage de l’interpréter. Dans le cas de graphiques, tels que les histogrammes ou les nuages de points, l’extraction texte est complexe, car l’ordre de l’extraction est fondamental.Ensuite, un modèle multimodal peut être utilisé pour répondre à l’utilisateur. L’usage d’un modèle multimodal à l’inférence (au sens chatbot) offre la possibilité au développeur d’insérer des graphiques au contexte du modèle servant le chatbot. À l’heure actuelle, le temps d’inférence de ces modèles n’offre pas une expérience utilisateur suffisante, mais il est très probable que cela évolue dans les mois à venir.Figure 9 : Les différents cas d’utilisation du multimodal - SourceL’option 1 consiste à générer l’embedding de l’image et à donner l’image en contexte d’un modèle multimodal pour répondre.L’option 2 consiste à générer un résumé de l’image et collecter l’embedding de son résumé. L’usage du multimodal intervient pour l’insertion dans la base documentaire, mais pas pour la réponse à la question.L’option 3 est similaire à l’option 2, mais l’image d’origine est donnée en contexte du modèle multimodal pour fournir la réponse. À l’heure actuelle, l’option 2 est la plus propice, car le temps de réponse API d’un modèle multimodal est long. Ce temps peut être acceptable pour alimenter la base documentaire, mais sera déceptive pour l’utilisateur qui attend une réponse rapide.Reprenons notre tableau sous format image uniquement et ajoutons le document qui nous a servi d’accroche dans cet article :Figure 10 : Exemple de graphique - Taille du contexte selon les LLMsCe graphique est assez complexe à exploiter par OCR. Les barres verticales relatives au nombre de tokens maximum par LLM ne sont pas explicitées. Cependant, l’œil humain identifie les modèles Claude comme des modèles à 100K tokens de contexte. Le nombre de tokens du modèle bloom est plus dur à quantifier à l’œil nu. Essayons d’extraire le texte de ce graphique avec Unstructured :L’OCR n’est pas performant du tout dans ce cas. Cependant, nous avons ajouté un paramètre afin de récupérer les images détectées dans le document. Elles sont stockées dans notre dossier data/pdf/extracted_images:Figure 11: Figure détectée et sauvegardée par Unstructured. Notez que le titre a été retiré.Il est donc possible d’automatiser le processus de synthèse des images trouvées dans le document. Voici la méthode et un exemple de prompt pour tenter de résumer une image en un tableau.Le résumé de GPT 4 Vision pour la Figure 11 est le suivant:Figure 12 : Résumé de l’image trouvé dans le document PDF correspondant à la Figure 10Que s’est-il passé ?Unstructured détecte et sauvegarde les graphiques présents dans les PDFs mais n’inclut pas le contexte environnant tel que les titres, commentaires ou justifications. La description faite de l’image est le nombre de paramètres par moteur de langage et non pas le nombre de token maximum.La détection de l’image réalisée par Unstructured s’avère donc inadaptée à une exploitation pour synthèse par le biais d’un modèle multimodal, car elle est trop réductrice. Cependant, Unstructured nous donne une information précieuse, la page du document qui contient l’image. Rien ne nous empêche de collecter la page complète qui contient l’image, à savoir l’image donnée par la Figure 11 incluant le titre du graphique. Le résumé fourni par GPT 4 Vision pour cette image titrée est le suivant.Figure 13: Résumé de la Figure 11 en utilisant GPT 4 VisionLes résultats sont plus performants. GPT 4 Vision s’est aidé du contexte environnant pour décrire le nombre de token par modèle. Ce dernier étant représenté par des barres verticales, il en a fait des approximations. Il faut toutefois noter que ce type de graphes est assez complexe même pour l’œil humain.En conclusion, pour l’exploitation des graphiques images, l’usage du multimodal peut être très utile. Sa mise en place consiste en plusieurs étapes :Nous détaillons ces différentes étapes dans ce notebook.Astuce: Généralement, les interfaces chatbots incluent les sources permettant à l’utilisateur de confirmer et d’approfondir sa recherche. Elles s’accompagnent parfois du texte extrait dans la documentation. En quick win, nous vous conseillons d’inclure les images et non pas leur résumé lorsque celles-ci sont utiles au retriever afin d’améliorer l’expérience visuelle de l’utilisateur.Le lecteur “Data Engineer” pourrait frissonner en lisant cette partie et il n’aurait pas tort. Les PDFs peuvent contenir un volume très importants d’images, dont la plupart ne nécessitent pas un résumé. Le coût computationnel et monétaire du multimodal n’est pas négligeable et il est important de pouvoir le contrôler. Cette dernière partie vous propose quelques astuces pour identifier les images pertinentes.Nous avons déjà expérimenté pour nos clients la synthèse visuelle des images présentes dans les documents PDF et PPT. Cependant, la grande majorité d’entre elles ne nécessitent pas d’être résumées. Les logos, photos et icônes embellissant un document peuvent représenter jusqu’à 90% des images de ce document.Afin de limiter les appels API aux modèles multimodaux, une première étape de filtrage des images est nécessaire. Notre objectif est d’analyser chaque image avec un modèle peu coûteux en ressources et d’identifier si elle apporte des informations utiles à notre chatbot. Nous supposerons dans cette partie qu’une image contenant un graphique ou un tableau est intéressante.Comment classifier une image en tant que tableau ou graphique ?La conférence International d’analyse et reconnaissance de document (ICDAR) a déjà abordé ce type de problématiques. Ce papier présente les différents modèles, datasets et leur performance pour la classification de graphiques.Certains modèles de classification en zero-shot learning peuvent aussi être utilisés. Cette tâche de classification étant relativement simple, des petits modèles suffiront. Parmi ces modèles, nous citerons CLIP et sa version Hugging Face très simple d’accès.Essayons de classifier les images extraites d’un PDF pour ne conserver uniquement les graphiques et les tableaux. Nous utiliserons le support présentation du Comptoir RAG animé par Nicolas Cavallo en janvier dernier chez Octo. Ce document contient 35 pages et inclut des graphiques, tableaux et autres images n’ayant pas d’intérêt à être résumées.Commençons par extraires les images et le texte avec UnstructuredNotre dossier d’images extraites contient 211 images. Essayons de réduire le nombre d’images à résumer à l’aide de CLIP. Nous utilisons içi une approche de zero-shot learning. Nous indiquons au modèle CLIP les labels possibles pour chaque image collecté de notre document: graph, table, other:Nous obtenons pour chaque image les probabilités, les valeurs softmax du produit scalaire en l’image et le label. Regardons par exemple les images associées aux labels graph ayant une probabilité supérieure à 90%.Figure 14: Images détectées comme ‘graphique’ par CLIP avec une probabilité supérieure à 90%Dans cet exemple, CLIP nous permet de détecter 11 images (dont 7 sont informatives). Ces images peuvent être résumées par un modèle multimodal comme Gemini ou GPT 4 Vision. Pour information, le temps de classification pour une image par CLIP est de 0.25 secondes contre 20 à 30 secondes pour GPT 4 Vision.Nous avons filtré 95% des images détectées par Unstructured sur un PDF de 53 pages. Le tableau suivant résume les temps et coûts d’ingestion approximatifs avec et sans cette étape de filtrage. Le filtre CLIP nous permet de réduire le temps d’ingestion d’un document de 90% dans notre exemple. Dans le cas où nous résumons la page du PDF contenant l’image et non pas l’image en question, afin de conserver le contexte environnant, le gain est de 65%. Retrouvez ici l’intégralité du code de cette section.Figure 15: Temps d’ingestion d’un document de 53 pages avec 211 images. L’analyse est faîte sur 2 CPU et sans parallélisme sur les appels APIs au modèle visionAfin de fine tuner un modèle pour améliorer sa détection, il nous faut un dataset d’entraînement. Notre problématique réside en la recherche d’images pertinentes dans des documents non structurés. Nous considérons une fois de plus que les images pertinentes sont les tableaux et les graphiques. Nous aurons toujours pour objectif de prédire si une image est un graphique, un tableau, ou une autre image. Ce papier, cité dans la partie précédente, présente différents datasets d’images de graphiques labellisés et notamment le dataset UB PMC. Il existe aussi des dataset de tableaux tels que le Table Bank dataset. Ces datasets sont initialement conçus pour entraîner des modèles de détection de structures et d’interprétations de graphiques. Nous les utiliserons uniquement à des fins de classification.Le choix du modèle à fine tuner est relatif à notre problématique, à savoir une classification. Vous pourrez utiliser un des modèles les plus performants et le fine tuner aux graphiques de votre propre entreprise ou à ceux issus des datasets cités précédemment.Voici un exemple de labels pour le dataset Doc Figure contenant des graphiques et des tableaux:Figure 16: Exemples d’images et catégories du dataset Doc FigureNous vous avons présenté dans cet articles différentes approches d’extractions d’informations non textuelles. La figure 17 synthétise les approches qui s’offrent à vous afin d’améliorer la qualité de votre base documentaire et du chatbot sous-jacent.Figure 17: Résumé des techniques présentées dans cet article pour l’exploitation des images et tableauxCes techniques peuvent complexifier et allonger la durée d’ingestion des documents dans votre base de données vectorielle. Cependant, selon le volume d’images et tableaux contenus dans vos documents, les mettre en place peut grandement améliorer la performance de votre Chatbot. Au cours de la rédaction de cet article, Llama Index à sorti Llama Parse, un parser de documents permettant de synthétiser les images et figures, utilisable par clé API uniquement pour l’instant.Sources:</td> </tr> </tbody> </table>]]></content><author><name></name></author><summary type="html"><![CDATA[Comment détecter, choisir, analyser et résumer des informations non textuelles (images & tableaux) dans vos sources documentaires en utilisant le multimodal ?]]></summary></entry><entry><title type="html">Construire son RAG (Retrieval Augmented Generation) grâce à langchain: L’exemple de l’Helpdesk d’OCTO - OCTO Talks !</title><link href="https://bastinflorian.github.io/blog/2023/construire-son-rag-retrieval-augmented-generation-grce-langchain-lexemple-de-lhelpdesk-docto-octo-talks/" rel="alternate" type="text/html" title="Construire son RAG (Retrieval Augmented Generation) grâce à langchain: L’exemple de l’Helpdesk d’OCTO - OCTO Talks !"/><published>2023-10-17T00:00:00+00:00</published><updated>2023-10-17T00:00:00+00:00</updated><id>https://bastinflorian.github.io/blog/2023/construire-son-rag-retrieval-augmented-generation-grce--langchain-lexemple-de-lhelpdesk-docto---octo-talks-</id><content type="html" xml:base="https://bastinflorian.github.io/blog/2023/construire-son-rag-retrieval-augmented-generation-grce-langchain-lexemple-de-lhelpdesk-docto-octo-talks/"><![CDATA[<p>“Je souhaiterais connaître la démarche pour accéder à mon comité d’entreprise”Qui n’a jamais imaginé poser cette question à un agent conversationnel permettant de lui indiquer facilement la source d’information, plutôt que de chercher dans 10 documents ou de demander à 10 personnes différentes ? Bienvenue au pays des RAG.Figure 1: Exemples de questions posées à notre ChatbotLe RAG, ou Retrieval Augmented Generation, nous permet d’utiliser la puissance d’un agent conversationnel en utilisant nos propres données.Nous développerons à travers cet article les différentes étapes permettant de créer notre Chatbot interne Octo. Cet agent explore la base de documentation Confluence (utilisée par OCTO pour ses activités internes) afin de simplifier la recherche utilisateur et répondre aux questions courantes des employés. Le code détaillé dans cet article est accessible sur ce repository Git.Comment exploiter un modèle de langage ?Le 16 mars dernier, Sam Altman met en lumière un aspect important de l’exploitation des modèles de langues:”The right way to think of the models that we create is a reasoning engine, not a fact database. They can also act as a fact database, but that’s not really what’s special about them – what we want them to do is something closer to the ability to reason, not to memorize.”Sam Altman - fondateur OpenAI Interview ABC - 16 MarsIl propose d’utiliser un agent conversationnel comme un moteur de raisonnement et non pas comme une base de connaissances. Autrement dit, les modèle de langages ne sont pas une immense base de données à la connaissance infinie qui résout nos problèmes en piochant dans son puit de savoir.Les modèles de langues sont initialement entraînés pour prédire le prochain mot d’une phrase. Grâce à des méthodes d’alignement (ici RLHF), la qualité des réponses de ces agents s’est grandement améliorée.Cependant, ces modèles ont aussi tendance à halluciner ou apporter des réponses biaisées. Afin de remédier à ces problèmes et pour personnaliser la réponse d’un modèle de langue, plusieurs solutions ont récemment émergé. Parmi ces solutions, il existe le prompting, ou l’art d’adapter notre question afin d’orienter le modèle vers une réponse souhaitée.Voici un exemple de prompt intéressant pour une problématique de Question Answering :Dans cet exemple, le modèle de langue a pour objectif de répondre à une question selon le contexte qui lui est transmis.Il existe aussi d’autres techniques de “prompting” pour résoudre des problèmes plus complexes tels que les Chain-of-Thought dont l’idée est d’introduire un raisonnement par étape. Comme nous allons le voir, ces étapes de “prompting” seront de plus en plus automatisées.DéfinitionLe RAG est une technique de traitement du langage naturel combinant la recherche d’informations dans une base documentaire et l’exploitation d’un moteur de langage. Il a pour objectif de répondre à des questions issues d’une documentation privée ou inconnue des moteurs de langages open ou closed-source.Pourquoi utiliser l’approche RAG et ne pas donner toute ma base documentaire au modèle ?Pour deux raisons :À titre d’exemple, le modèle ChatGPT autorise une taille de contexte allant jusqu’à 32,768 tokens. Pour comparaison, le roman de Georges Orwell, 1984, a une taille de 89 000 tokens.Il est donc impossible de fournir toute une documentation d’entreprise en contexte d’un modèle de langue. Pour comprendre ce qu’est un token, voici un exemple de découpage ou tokenisation selon Chat GPT:Alors, comment choisir les documents à lui transmettre ?Le RAG est constitué d’une suite d’étapes permettant d’établir cette sélection :La création de chunks (la division du corpus de textes en sous-parties)La création d’embeddings (la transformation de ces sous-parties en vecteurs de valeurs numériques)La création d’une base de données vecteur (le stockage de ces valeurs numériques dans une base de données adaptée)La recherche d’informations ou information retrieval (la recherche des chunks sémantiquement proches de la question posée)Ces étapes sont requises pour mener à bien l’indexation de notre base documentaire.Commençons par étudier comment stocker notre documentation pour la rendre exploitable :Dans un premier temps, la base de données de l’entreprise est extraite (.pdf, .md, .txt, …) et divisée en sous-parties.  Chaque sous-partie peut être de taille fixe ou variable selon le choix de découpage des documents. Des techniques d’overlap sont possible mais peuvent introduire des doublons lors du choix des chunks adéquats pour la réponse à une question posée.Ensuite, ces sous-parties sont converties en embeddings, décrivant l’information sémantique qu’elles contiennent.Enfin, ces embeddings sont stockés dans une base de données vecteur.Figure 2: L’ingestion de la base de données de l’entrepriseEnsuite, intéressons-nous à la relation entre la base de données vecteurs et notre modèle de langue.Une fois la base de données vecteur créée, notre documentation d’entreprise est exploitable. L’utilisateur peut alors poser une question:Comment accéder à mon comité d’entreprise ?Cette question est convertie en un vecteur contenant son embedding, représentant l’information sémantique de la question. À partir de l’embedding de la question, une recherche d’embedding similaire est faite dans la base de données vecteur.  L’objectif de cette recherche est de retrouver les parties de documents en relation avec la question. De cette recherche, deux contenus ressortent :Ces contenus forment le contexte et seront ajoutés à notre prompt:Le template de prompt:Se complète avec le contexte et la question:Ce prompt final est mis en entrée d’un modèle de langue, voici sa réponse :Et voilà ! Nous venons d’ajuster la question que nous posons à notre modèle de langue pour que sa réponse soit personnalisée par rapport à nos données.A titre de comparaison: imaginez que vous souhaitez connaître les destinations de vacances éco-responsables qui vous conviendront. Vous pouvez imaginer poser une question sur l’interface publique de Chat GPT en lui demandant quelles sont les stations balnéaires éco-responsables. Votre lieu de vie étant important pour obtenir une bonne réponse, votre cerveau utilisera probablement implicitement l’approche RAG, en orientant le modèle vers la réponse adéquate grâce à l’indication de votre localisation.C’est l’objectif du RAG, piocher parmi vos informations personnelles ou d’entreprise pour mieux vous répondre. La question ‘Ou puis-je partir en vacances dans un lieu éco-responsable ?’ se transformera en ‘Sachant que j’habite à Paris, ou puis-je partir en vacances dans un lieu éco-responsable ?Figure 2: Architecture RAGUne approche RAG pour la recherche de documentation requiert la construction d’un pipeline, comme présenté ci-dessus. Pour simplifier la création de ces pipelines, communément appelé chaînes, Harrison Chase crée en Octobre 2022 LangChain.LangChain est un framework simplifiant la création de chaînes, ou suite d’étapes, permettant entre autres la mise en place de chatbots basés sur les modèles de langue.Grâce à sa large communauté Open Source, LangChain offre de nombreuses fonctionnalités et est aujourd’hui le moyen le plus efficace pour créer des architectures autour des modèles de langues. Parmi les fonctionnalités de LangChain, nous citerons les suivantes:Document Loader : permet de lire des textes d’une sourceVector Store : base de données vectorielle pour les embeddingsRetriever : outils pour retrouver des documents dans une DB ou sur le WebChain : pipeline de traitement LLM, en plusieurs étapesTools: un moyen de connecter un agent avec des services extérieurs - équivalent des plugins de ChatGPTAgent: extension des chaînes pour ajouter des outils (lien avec l’extérieur du LLM)Passons maintenant à l’implémentation d’un Chatbot pour répondre à des questions sur les données de notre entreprise.Chez Octo la documentation de l’entreprise est sur Confluence. Elle regroupe entre autres des informations sur les outils internes, les réponses aux questions des employés et des retours d’expériences sur des sujets techniques. Il arrive que l’information soit dispersée à différents endroits, ou que les titres des documentations ne soient pas explicites.Pour résoudre ces limites et proposer une solution user-friendly, nous avons choisi de créer un Chatbot conversationnel répondant aux questions des employés en indiquant les sources utiles pour approfondir le sujet.Nous présentons ci-dessous les différentes étapes et le code LangChain pour la création de notre solution : Le Help Desk.Vous trouverez le repository Github ici.La première étape consiste à charger les données depuis Confluence.LangChain propose différents types de Loader selon le type de fichier (.pdf, .txt, .md, .png). La documentation Octo dans Confluence est en Markdown (le loader Markdown inclut une fonctionnalité d’extraction de textes dans les images).Nous utilisons le Confluence Loader afin de collecter la documentation directement avec une clé API. Nous avons aussi proposé une MR à LangChain, validée sur la version 0.0.245, pour conserver la structure Markdown et obtenir une meilleure segmentation des chunks.python<br/>import markdownify<br/>from langchain.document_loaders import ConfluenceLoader<br/><br/>loader = ConfluenceLoader(<br/> url=CONFLUENCE_SPACE_NAME, # ex: https://prenomnom.atlassian.net/wiki<br/> username=CONFLUENCE_USERNAME, # ex: monentreprise@gmail.com<br/> api_key=CONFLUENCE_API_KEY # Create a key here<br/>)<br/><br/>docs = loader.load(<br/> space_key=CONFLUENCE_SPACE_KEY, # <space_name>/spaces/<space_key>/overview<br/> keep_markdown_format=True<br/>)<br/><br/>print(docs[-1].page_content)<br/><br/>&gt;&gt; <br/>“””<br/>...<br/>## Comment accéder à mon CE ?<br/><br/>Pour accéder au CE, vous pouvez consultant l’adresse suivante: &lt;https://mon-CE.fr&gt;<br/><br/>Si vous n’avez pas vos identifiants, vous pouvez envoyer un mail à xxxx@mon-ce.fr, le responsable du comité d’entreprise. <br/><br/>Renseignez les identifiants sur le site du CE et vous pourrez bénéficier de nombreux avantages.<br/><br/>## Rôle du Comité d'Entreprise :<br/><br/>Le Comité d'Entreprise joue un rôle essentiel dans le dialogue social au sein de l'entreprise. Ses principales missions sont les suivantes :<br/><br/>### 1. Défense des droits des salariés :<br/><br/>Le Comité d'Entreprise est chargé de veiller au respect des droits des salariés en matière de travail, de sécurité, de santé et de conditions de travail.<br/>...<br/>“””<br/>Comme expliquée dans cet article de Pinecone, l’étape de création des chunks est stratégique. L’objectif est de conserver l’intégralité du contexte dans un chunk afin que sa représentation vectorielle soit la plus représentative possible.Nos données textuelles ayant été récupérées au format Markdown, il est possible d’en distinguer les parties et sous-parties. On espère maximiser la qualité de nos chunks et des embeddings qui les représenteront dans l’étape suivante.Le Markdown Header Text Splitter découpe notre page Confluence en identifiant les titres et sous-titres de notre page Markdown. En plus d’un découpage intelligent, les métadonnées des chunks sont complétées avec les titres et sous-titres trouvés.Prenons un exemple avec le découpage d’un article.python<br/>from langchain.text_splitter import MarkdownHeaderTextSplitter<br/><br/># Markdown <br/>headers_to_split_on = [<br/> ("#", "Titre"),<br/> ("##", "Sous-titre 1"),<br/> ("###", "Sous-titre 2"),<br/>]<br/><br/># Markdown splitter<br/>markdown_splitter = MarkdownHeaderTextSplitter(<br/> headers_to_split_on=headers_to_split_on<br/>)<br/><br/>chunks = markdown_splitter.split_text(docs[1].page_content) # Sample one doc<br/><br/>pretty_print(chunks)<br/><br/>"""<br/>...<br/>Pour accéder au CE, vous pouvez consultant l’adresse suivante: &lt;https://mon-CE.fr&gt; <br/>Si vous n’avez pas vos identifiants, vous pouvez envoyer un mail à xxxx@mon-ce.fr, le responsable du comité d’entreprise. <br/>Renseignez les identifiants sur le site du CE et vous pourrez bénéficier de nombreux avantages.<br/>--------------------------------------------------<br/>{'Sous-titre 1': 'Comment accéder à mon CE ?'}<br/>==================================================<br/>Le Comité d'Entreprise joue un rôle essentiel dans le dialogue social au sein de l'entreprise. Ses principales missions sont les suivantes :<br/>--------------------------------------------------<br/>{'Sous-titre 1': "Rôle du Comité d'Entreprise :"}<br/>==================================================<br/>Le Comité d'Entreprise est chargé de veiller au respect des droits des salariés en matière de travail, de sécurité, de santé et de conditions de travail. Il peut être consulté par la direction de l'entreprise sur différentes questions liées à l'organisation du travail, aux licenciements collectifs, aux restructurations, etc. Il est également informé des projets de l'entreprise et peut émettre des avis.<br/>--------------------------------------------------<br/>{'Sous-titre 1': "Rôle du Comité d'Entreprise :", 'Sous-titre 2': '1. Défense des droits des salariés :'}<br/>...<br/>"""<br/>Que se passe-t-il si les chunks créés suivant la structure de notre page Confluence sont trop longs ?Nous affinons ensuite ce premier découpage en utilisant le Recursive Character Text Splitter. Cette méthode permet de re-diviser de manière récursive les blocs de textes encore trop longs après le premier découpage.Dans l’exemple ci-dessous, la partie 1. La défense des droits des salariés est divisée en deux chunks.python<br/>from langchain.text_splitter import RecursiveCharacterTextSplitter<br/><br/>splitter = RecursiveCharacterTextSplitter(<br/> chunk_size=400,<br/> chunk_overlap=20,<br/> separators=['\n\n', '\n', '(?&lt;=. )', ' ', '']<br/>)<br/><br/>splitted_chunks = splitter.split_documents(chunks)<br/>pretty_print(splitted_chunks)<br/><br/>"""<br/>...<br/>Le Comité d'Entreprise est chargé de veiller au respect des droits des<br/>salariés en matière de travail, de sécurité, de santé et de conditions de<br/>travail. <br/>Il peut être consulté par la direction de l'entreprise sur différentes questions liées <br/>à l'organisation du travail, aux licenciements collectifs, aux restructurations, etc.<br/>--------------------------------------------------<br/>{'Sous-titre 1': "Rôle du Comité d'Entreprise :", 'Sous-titre 2': '1. Défense des droits des salariés :'}<br/>==================================================<br/>Il est également informé des projets de l'entreprise et peut émettre des avis.<br/>--------------------------------------------------<br/>{'Sous-titre 1': "Rôle du Comité d'Entreprise :", 'Sous-titre 2': '1. Défense des droits des salariés :'}<br/>...<br/>"""<br/>Le repository Github inclut des fonctionnalités additionnelles qui ne sont pas présentées ici :L’ajout des métadonnées de la page Confluence (url, id) avec les métadonnées des blocs de textes (titre, sous-titre)L’ajout des titres et sous-titres au texte du documentLes fonctions vous permettant de tester le découpage de bout en boutUne interface StreamlitLa troisième étape consiste à transformer nos morceaux de textes en représentations vectorielles, appelées embeddings. Ces embeddings nous permettront de faire une étude de similarité pour affiner le contexte transmis au modèle de langue. Il doivent cependant être stockés dans une base de données vecteurs.LangChain propose de nombreuses intégrations de modèles d’embeddings et de bases de données vecteurs. Nous utilisons par défaut le modèle embedding d’Open AI, text-embedding-ada-002. Nous choisissons la base de données vecteur légère Chroma que LangChain met à dispositionAttention cependant avec le choix des modèles de langage. Utiliser un modèle en utilisant une API tiers tel que celle de Chat GPT rend votre donnée publique. Pour se prémunir de la divulgation de données deux solutions s'offrent à vous:Un prochain article détaillera comment remplacer l’API Open AI par des modèles Open Source.python<br/># Embeddings and vector store<br/>import shutil<br/>from langchain.vectorstores import Chroma<br/>from langchain.embeddings import OpenAIEmbeddings<br/><br/>persist_directory = './db/chroma'<br/>chunks = my_custom_splitter(docs) # See notebook for function definition<br/>embeddings = OpenAIEmbeddings() # Default to text-embdding-ada-002<br/><br/># If the directory exists, first delete it<br/>try:<br/> shutil.rmtree(persist_directory)<br/>except FileNotFoundError as e:<br/> pass<br/><br/># Create vector store and save the db<br/>db = Chroma.from_documents(<br/> chunks, <br/> embeddings,<br/> persist_directory=persist_directory<br/>)<br/>db.persist()<br/>Dans cette partie, nous allons créer une chaîne nous permettant de mettre en place notre processus de question/réponse.Nous commençons par créer le retriever qui permet de retrouver les chunks de notre base vecteur. Parmi les paramètres possibles, nous pouvons spécifier une mesure de similarité, indiquer le nombre de chunks souhaités en sortie, ou un score minimum de similarité par rapport à la question posée.Ces décisions ont un impact sur la qualité des réponses et doivent être optimisées. Le nombre de chunks dépend de différents paramètres tel que le niveau de granularité du découpage ou la richesse de la documentation. Le score de similarité dépend de la proximité des chunks avec la question et donc du niveau de précision de la base documentaire.python<br/>retriever = db.as_retriever(<br/> search_type="similarity_score_threshold", <br/> search_kwargs={<br/> "k": 5, <br/> "score_threshold": 0.3<br/> }<br/>)<br/>Il convient ensuite de créer le prompt adéquat qui aidera le modèle de langue à raisonner.</space_key></space_name></p>]]></content><author><name></name></author><summary type="html"><![CDATA[“Je souhaiterais connaître la démarche pour accéder à mon comité d’entrepriseQui n'a jamais imaginé poser cette question à un agent conversationnel permettant de lui indiquer facilement la source d'information, plutôt que de chercher dans 10 documents ou de demander à 10 personnes différentes ? Bienvenue au pays des RAG.Figure 1: Exemples de quest...]]></summary></entry><entry><title type="html">How to balance power and control when using Dataflow with an OLTP SQL Database | Beam Summit</title><link href="https://bastinflorian.github.io/blog/2023/how-to-balance-power-and-control-when-using-dataflow-with-an-oltp-sql-database-beam-summit/" rel="alternate" type="text/html" title="How to balance power and control when using Dataflow with an OLTP SQL Database | Beam Summit"/><published>2023-07-14T00:00:00+00:00</published><updated>2023-07-14T00:00:00+00:00</updated><id>https://bastinflorian.github.io/blog/2023/how-to-balance-power-and-control-when-using-dataflow-with-an-oltp-sql-database--beam-summit</id><content type="html" xml:base="https://bastinflorian.github.io/blog/2023/how-to-balance-power-and-control-when-using-dataflow-with-an-oltp-sql-database-beam-summit/"><![CDATA[<p>We created a Python SDK-based Dataflow streaming pipeline for a major French retail company. When notified, the pipeline efficiently reads large CSV files from Google Cloud Storage and selects, inserts, upserts, and deletes rows from a Cloud SQL Postgres database with a controlled number of connections.The business purpose of this project is to use streaming queries in order to apply various types of transactions to an OLTP database based on CSV files.Technical description:Connecting Cloud SQL to Dataflow is not straightforward. For example, the Cloud SQL JDBC connector is limited in the kind of read and write operations it allows and other custom connectors and can be easily overflown due to the parallelism and autoscaling capabilities of Apache Beam and Dataflow. Additionally, since the number of connections for a database is limited, we developed additional features to prevent connections from being overwhelmed.Main focus:After reviewing the most common ways to control the level of parallelism and its limit (number of threads and workers&amp;mldr;), our talk will focus on how we controlled the number of connections to Cloud SQL in a Dataflow pipeline by leveraging the beam.utils.Shared module to share connections at the worker level.We will show that by doing that and using the different flavors of reshuffle based transforms (groupIntoBatches, GroupByKey&amp;mldr;), you can achieve a better control of your SQL connections.We also developed SDF for reading large CSV files and created a streaming pipeline for inserting CSV rows into an OLTP database. Since the connection between Dataflow and Cloud SQL is not highlighted in the Google and Beam documentation, we want to share our experience with other companies who faced similar issues at the summit.We created a Python SDK-based Dataflow streaming pipeline for a major French retail company. When notified, the pipeline efficiently reads large CSV files from Google Cloud Storage and selects, inserts, upserts, and deletes rows from a Cloud SQL Postgres database with a controlled number of connections.The business purpose of this project is to use streaming queries in order to apply various types of transactions to an OLTP database based on CSV files.Technical description:Connecting Cloud SQL to Dataflow is not straightforward. For example, the Cloud SQL JDBC connector is limited in the kind of read and write operations it allows and other custom connectors and can be easily overflown due to the parallelism and autoscaling capabilities of Apache Beam and Dataflow. Additionally, since the number of connections for a database is limited, we developed additional features to prevent connections from being overwhelmed.Main focus:After reviewing the most common ways to control the level of parallelism and its limit (number of threads and workers&amp;mldr;), our talk will focus on how we controlled the number of connections to Cloud SQL in a Dataflow pipeline by leveraging the beam.utils.Shared module to share connections at the worker level.We will show that by doing that and using the different flavors of reshuffle based transforms (groupIntoBatches, GroupByKey&amp;mldr;), you can achieve a better control of your SQL connections.We also developed SDF for reading large CSV files and created a streaming pipeline for inserting CSV rows into an OLTP database. Since the connection between Dataflow and Cloud SQL is not highlighted in the Google and Beam documentation, we want to share our experience with other companies who faced similar issues at the summit.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The Beam Summit brings together experts and community to share the exciting ways they are using, changing, and advancing Apache Beam and the world of data and stream processing.]]></summary></entry></feed>