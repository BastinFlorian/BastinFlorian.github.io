<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Generative AI Quiz | Florian Bastin </title> <meta name="author" content="Florian Bastin"> <meta name="description" content="Generative AI Quiz for Dauphine Tunis students"> <meta name="keywords" content="GenAI, RAG, LLM, Daupgine Tunis"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/favicon.jpg?65c4f4ef6f7c54ebb97a7d744048083d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bastinflorian.github.io/2024-10-18-genai-quiz/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Florian</span> Bastin </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Git Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/readings/">Essential LLM Readings </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Generative AI Quiz</h1> <p class="post-meta"> Created on October 18, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-hashtag fa-sm"></i> GenAI Quiz Dauphine-Tunis Teaching   ·   <i class="fa-solid fa-tag fa-sm"></i> Teaching </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="quiz-1---building-large-language-models">Quiz 1 - Building Large Language Models</h2> <h3 id="questions">Questions</h3> <p>Question 1: <strong>What does LLM mean?</strong></p> <ul> <li>A) Linguistic Laugh Machine</li> <li>B) Legendary Lemonade Maker</li> <li>C) Large Language Model</li> <li>D) Levitating Lawn Mower</li> </ul> <p>Question 2: <strong>What is tokenization?</strong></p> <ul> <li>A) The process of encrypting text data for security purposes.</li> <li>B) The method of assigning numerical frequency values to words.</li> <li>C) The process of breaking down text into individual units called tokens.</li> <li>D) The technique of summarizing large texts into key points.</li> </ul> <p>Question 3: <strong>Which metric is commonly used to evaluate the performance of a language model?</strong></p> <ul> <li>A) Accuracy Score</li> <li>B) Perplexity</li> <li>C) Recall Rate</li> <li>D) F1 Score</li> </ul> <p>Question 4: <strong>How are language models generally trained?</strong></p> <ul> <li>A) By manually programming grammar rules into the system.</li> <li>B) By feeding the model a vast set of text data to predict the next word.</li> <li>C) By translating texts between languages to improve linguistic understanding.</li> <li>D) By analyzing and replicating human brain activity patterns.</li> </ul> <p>Question 5: <strong>What is supervised fine-tuning (SFT)?</strong></p> <ul> <li>A) Compressing a language model to reduce computational requirements</li> <li>B) Refining a pre-trained LLM with task-specific, human-annotated data</li> <li>C) Training a language model from scratch using only labeled data</li> <li>D) Adjusting a language model’s outputs based on automated feedback loops</li> </ul> <p>Question 6: <strong>What is one of the challenges when using perplexity as an evaluation metric?</strong></p> <ul> <li>A) It requires labeled datasets, which are hard to obtain</li> <li>B) It only measures the speed of the model, not accuracy</li> <li>C) It depends on the vocabulary size</li> <li>D) It cannot be computed for large language models</li> </ul> <p>Question 7: <strong>Which of the following is a typical step in data preprocessing for large language models (LLMs)?</strong></p> <ul> <li>A) Encrypting all textual data to protect privacy</li> <li>B) Injecting noise into data to test model robustness</li> <li>C) Removing duplicates and low-quality data from the dataset</li> <li>D) Translating all data into a single language</li> </ul> <p>Question 8: <strong>What does the Chinchilla scaling law propose?</strong></p> <ul> <li>A) Increasing layers indefinitely improves performance</li> <li>B) Balancing model size and training data for optimal performance</li> <li>C) Always train the largest model, regardless of data amount</li> <li>D) Using less computational resources enhances generalization</li> </ul> <p>Question 9: <strong>What do scaling laws propose?</strong></p> <ul> <li>A) Doubling data size halves training time</li> <li>B) Performance improves predictably with larger models and more data</li> <li>C) Smaller models are more efficient than larger ones</li> <li>D) Less training data leads to better generalization</li> </ul> <p>Question 10: <strong>What is Direct Preference Optimization (DPO)?</strong></p> <ul> <li>A) Uses a reward model and reinforcement learning</li> <li>B) Directly optimizes model outputs based on human feedback</li> <li>C) Augments data by reversing text sequences</li> <li>D) Compresses models to reduce computation</li> </ul> <p>Question 11: <strong>What is a main challenge in evaluating LLMs?</strong></p> <ul> <li>A) Answer preference is not trivial</li> <li>B) LLMs agree with themselves only 66% of the time</li> <li>C) Humans have a lot of variance</li> <li>D) Metrics capture all language aspects perfectly</li> </ul> <h3 id="solutions">Solutions</h3> <ul> <li>Solution 1: C</li> <li>Solution 2: C</li> <li>Solution 3: B</li> <li>Solution 4: B</li> <li>Solution 5: B</li> <li>Solution 6: C</li> <li>Solution 7: C</li> <li>Solution 8: B</li> <li>Solution 9: B</li> <li>Solution 10: B</li> <li>Solution 11: A</li> </ul> <h2 id="quiz-2---before-transformers">Quiz 2 - Before Transformers</h2> <h3 id="questions-1">Questions</h3> <p>Question 1: <strong>What is an N-gram in language modeling</strong></p> <ul> <li>A) A neural network component.</li> <li>B) A statistical sequence of N items.</li> <li>C) A word embedding technique.</li> <li>D) A parsing algorithm.</li> </ul> <p>Question 2: <strong>Word embeddings are sparse vectors.</strong></p> <ul> <li>A) True</li> <li>B) False</li> </ul> <p>Question 3: <strong>How do recurrent neural networks (RNNs) work?</strong></p> <ul> <li>A) Process inputs independently</li> <li>B) Use internal states to handle sequences</li> <li>C) Apply convolution over data</li> <li>D) Utilize attention mechanisms</li> </ul> <p>Question 4: <strong>What is the limitation of RNNs in language modeling?</strong></p> <ul> <li>A) Difficulty learning long-term dependencies</li> <li>B) Suffering from vanishing gradient problem</li> <li>C) Cannot be implemented using GPU for acceleration</li> <li>D) Cannot handle variable-length sequences</li> </ul> <p>Question 5: <strong>LSTM models improve upon RNNs by</strong></p> <ul> <li>A) Requiring less computational power than RNN</li> <li>B) Solving the vanishing gradient problem</li> <li>C) Introducing the gating mechanism for long-term dependencies</li> <li>D) Not overfitting due to their architectural improvements over RNN</li> </ul> <p>Question 6: <strong>How do LSTMs solve the vanishing gradient problem?</strong></p> <ul> <li>A) By resetting the hidden state after each time step</li> <li>B) By avoiding using recurrent connections</li> <li>C) Using gate mechanisms to reduce the flow of information</li> <li>D) With the cell state allowing gradient to flow unchanged</li> </ul> <p>Question 7: <strong>How do LSTMs learn?</strong></p> <ul> <li>A) Using self-supervised learning for time series forecasting</li> <li>B) Using backpropagation through time</li> <li>C) Using supervised learning for next word prediction</li> <li>D) Using perplexity as a loss function</li> </ul> <h3 id="solutions-1">Solutions</h3> <ul> <li>Solution 1: B</li> <li>Solution 2: B</li> <li>Solution 3: B</li> <li>Solution 4: A, B</li> <li>Solution 5: B, C</li> <li>Solution 6: C, D</li> <li>Solution 7: B</li> </ul> <h2 id="quiz-3---transformers-architecture">Quiz 3 - Transformers Architecture</h2> <h3 id="questions-2">Questions</h3> <p>Question 1: <strong>What is the purpose of the self-attention mechanism?</strong></p> <ul> <li>A) Allow the model to focus on different parts of the input at inference.</li> <li>B) To capture positional information of tokens in a sequence.</li> <li>C) To reduce the computational complexity compared to RNNs.</li> <li>D) To enable parallel processing of sequence elements.</li> </ul> <p>Question 2: <strong>What is the role of multi-head attention in Transformer architectures?</strong></p> <ul> <li>A) Enable the model to focus on different sequence positions simultaneously.</li> <li>B) To increase the model’s capacity to learn different types of relationships.</li> <li>C) To decrease the overall computational cost of the model.</li> <li>D) To provide positional information to the model.</li> </ul> <p>Question 3: <strong>In the Transformer architecture, what is the purpose of positional encoding?</strong></p> <ul> <li>A) To allow the model to understand the order of the sequence elements.</li> <li>B) To replace the need for self-attention mechanisms.</li> <li>C) To improve the convergence rate during training.</li> <li>D) To provide additional input features for better performance.</li> </ul> <p>Question 4: <strong>What is the function of the scaling factor \frac{1}{\sqrt{d_k}} in the scaled dot-product attention?</strong></p> <ul> <li>A) To make computations more efficient.</li> <li>B) Ensure the softmax function operates in a highly sensitive region.</li> <li>C) Adjust dot product magnitude to prevent extremely small gradients.</li> <li>D) To reduce computational complexity.</li> </ul> <p>Question 5: <strong>In the Transformer architecture, what is the primary reason for using residual connections and layer normalization?</strong></p> <ul> <li>A) To facilitate the flow of gradients during training.</li> <li>B) To allow deeper networks without the vanishing gradient problem.</li> <li>C) To reduce overfitting on the training data.</li> <li>D) To decrease the computational requirements during inference.</li> </ul> <p>Question 6: <strong>Which of the following statements about the encoder and decoder in the Transformer architecture are true?</strong></p> <ul> <li>A) The encoder processes input sequence into continuous representations.</li> <li>B) The translation models use only the encoder part of the Transformers.</li> <li>C) The decoder does not use any attention mechanism.</li> <li>D) Both encoder and decoder use self-attention mechanisms.</li> </ul> <p>Question 7: <strong>What is the purpose of masked self-attention in the decoder?</strong></p> <ul> <li>A) Prevent the model from attending to future positions in the sequence during training.</li> <li>B) To allow the model to attend only to relevant parts of the input sequence.</li> <li>C) To enforce causality in sequence generation.</li> <li>D) To reduce the computational complexity of the attention mechanism.</li> </ul> <p>Question 8: <strong>What are the advantages of using the Transformer architecture over traditional RNN-based models?</strong></p> <ul> <li>A) Parallel processing of sequence elements, leading to faster training.</li> <li>B) Capture long-range dependencies more effectively.</li> <li>C) Fixed computational cost per time step regardless of the sequence length.</li> <li>D) Fewer parameters than RNNs.</li> </ul> <p>Question 9: <strong>In the context of Transformers, what is the role of the softmax function in the attention mechanism?</strong></p> <ul> <li>A) To normalize the attention scores into probabilities.</li> <li>B) To allow the model to focus on the input sequence’s most relevant parts.</li> <li>C) To reduce the dimensionality of the data.</li> <li>D) To introduce non-linearity into the model.</li> </ul> <p>Question 10: <strong>Which of the following correctly describe the key, query, and value vectors in the Transformer architecture?</strong></p> <ul> <li>A) They are linear projections of the input embeddings.</li> <li>B) The query vector represents the content of the current position.</li> <li>C) They are used to compute attention scores.</li> <li>D) The value vector is used to store the actual content to be aggregated.</li> </ul> <p>Question 11: <strong>In multi-head attention, why are separate linear transformations applied to queries, keys, and values for each head?</strong></p> <ul> <li>A) To allow each head to attend to different aspects of the input.</li> <li>B) To reduce overfitting by increasing parameter sharing.</li> <li>C) To increase model capacity without increasing computational cost too much.</li> <li>D) To ensure that all heads produce identical outputs.</li> </ul> <p>Question 12: <strong>Suppose that the embedding size is d_{\text{model}} = 512 and number of heads = 8. What are Q, K, V vectors dimensions d_k ?</strong></p> <ul> <li>A) 512</li> <li>B) 128</li> <li>C) 64</li> <li>D) 256</li> </ul> <h3 id="solutions-2">Solutions</h3> <ul> <li>Solution 1: A</li> <li>Solution 2: B</li> <li>Solution 3: A</li> <li>Solution 4: C</li> <li>Solution 5: A, B</li> <li>Solution 6: A, D</li> <li>Solution 7: A, C</li> <li>Solution 8: A, B</li> <li>Solution 9: A, B</li> <li>Solution 10: A, B, C, D</li> <li>Solution 11: A</li> <li>Solution 12: B</li> </ul> <h2 id="quiz-4---retrieval-augmented-generation">Quiz 4 - Retrieval Augmented Generation</h2> <h3 id="questions-3">Questions</h3> <p>Question 1: <strong>What are the primary components of a Retrieval-Augmented Generation (RAG) system?</strong></p> <ul> <li>A) A retrieval module to fetch relevant documents.</li> <li>B) A generative language model to produce outputs using retrieved documents.</li> <li>C) An encoder-decoder architecture for language translation.</li> <li>D) A reinforcement learning agent to optimize retrieval strategies.</li> </ul> <p>Question 2: <strong>In the context of RAG, what is the main purpose of using vector embeddings?</strong></p> <ul> <li>A) To represent text documents and queries in a high-dimensional space.</li> <li>B) To enable efficient similarity searches between queries and documents.</li> <li>C) To reduce the dimensionality of data for computational efficiency.</li> <li>D) To train the language model with fewer parameters.</li> </ul> <p>Question 3: <strong>Which of the following are advantages of using RAG over traditional language models?</strong></p> <ul> <li>A) Ability to provide up-to-date information not present in the training data.</li> <li>B) Reduction of hallucinations by grounding responses in retrieved documents.</li> <li>C) Increased computational efficiency due to smaller model sizes.</li> <li>D) Improved handling of long-range dependencies within sequences.</li> </ul> <p>Question 4: <strong>What are considered as the most powerful for retrieving documents?</strong></p> <ul> <li>A) TF-IDF</li> <li>B) BM25</li> <li>C) Cosine similarity</li> <li>D) Hybrid Search</li> </ul> <p>Question 5: <strong>Which retrieval method should I use for the query: “What is the name of the capital city of Joe Biden’s country?”</strong></p> <ul> <li>A) TD-IDF</li> <li>B) BM25</li> <li>C) Cosine Similarity</li> <li>D) Hybrid method</li> </ul> <p>Question 6: <strong>How does Self-RAG improve the performance of the RAG system?</strong></p> <ul> <li>A) By generating multiple possible response segments in parallel.</li> <li>B) By using a critic model to select the most accurate segment.</li> <li>C) By recursively summarizing retrieved documents.</li> <li>D) By adding a retrieval evaluator to assess source quality.</li> </ul> <p>Question 7: <strong>Which metrics are used to evaluate the retriever in a RAG system?</strong></p> <ul> <li>A) Precision at k</li> <li>B) Recall at k</li> <li>C) Normalized Discounted Cumulative Gain (NDCG)</li> <li>D) BLEU / ROUGE</li> </ul> <p>Question 8: <strong>What technique does RAG use to overcome context size limitations when processing large documents?</strong></p> <ul> <li>A) It ignores parts of the document that don’t fit in the context.</li> <li>B) It segments the document into smaller blocks based on titles or headings.</li> <li>C) It compresses documents to reduce their size.</li> <li>D) It increases the context size of the language model.</li> </ul> <p>Question 9: <strong>What does Corrective RAG (CRAG) add to the traditional RAG system?</strong></p> <ul> <li>A) A retrieval evaluator to assess the quality of retrieved sources.</li> <li>B) An integrated machine translation module.</li> <li>C) A critic model to select the best responses.</li> <li>D) A hierarchy of summaries to preserve context.</li> </ul> <p>Question 10: <strong>What is the primary function of RAPTOR?</strong></p> <ul> <li>A) Generates multiple responses in parallel and uses a critic model.</li> <li>B) Adds a retrieval evaluator to assess the quality of retrieved sources.</li> <li>C) Recursively summarizes retrieved documents, creating a hierarchy of summaries.</li> <li>D) Processes the entire dataset to create a knowledge graph.</li> </ul> <p>Question 11: <strong>What is the primary function of GraphRAG?</strong></p> <ul> <li>A) Generates multiple possible response segments in parallel</li> <li>B) Create a full dataset knowledge graph that organizes data hierarchically</li> <li>C) Creates a hierarchy of summaries to reduce information overload</li> <li>D) Adds a retrieval evaluator to assess the quality of retrieved sources</li> </ul> <p>Question 12: <strong>What is the primary function of HyDE?</strong></p> <ul> <li>A) It compresses large documents into shorter embeddings</li> <li>B) It creates a hierarchical structure of documents to improve retrieval</li> <li>C) It employs RLHF to achieve better retrieval</li> <li>D) It generates hypothetical answers to queries used for retrieving</li> </ul> <p>Question 13: <strong>What primary issue does the “Lost in the Middle” paper address?</strong></p> <ul> <li>A) Models cannot handle sequences longer than 512 tokens</li> <li>B) Models have increased computational costs with longer contexts</li> <li>C) Models focus on the beginning and end of contexts</li> <li>D) Models perform equally well across all parts of the context</li> </ul> <p>Question 14: <strong>What are HNSW and FAISS famous for?</strong></p> <ul> <li>A) Algorithms for compressing language models to reduce size</li> <li>B) Methods for training language models with fewer parameters</li> <li>C) Approximate nearest neighbor search for efficient retrieval of embeddings</li> <li>D) Techniques for augmenting training datasets with synthetic data</li> </ul> <p>Question 15: <strong>What is the primary purpose of Reciprocal Rank Fusion (RRF)?</strong></p> <ul> <li>A) Combine rankings from multiple retrieval models</li> <li>B) Fuse document embeddings into a single representation for better context</li> <li>C) Dynamically adjust retrieval strategies based on user interactions</li> <li>D) Prioritize documents with the highest retrieval latency</li> </ul> <h3 id="solutions-3">Solutions</h3> <ul> <li>Solution 1: A, B</li> <li>Solution 2: A, B</li> <li>Solution 3: A, B</li> <li>Solution 4: D</li> <li>Solution 5: D</li> <li>Solution 6: B, D</li> <li>Solution 7: A, B, C</li> <li>Solution 8: B</li> <li>Solution 9: A</li> <li>Solution 10: D</li> <li>Solution 11: C</li> <li>Solution 12: D</li> <li>Solution 13: C</li> <li>Solution 14: C</li> <li>Solution 15: A</li> </ul> <h2 id="quiz-5---beyond-llm-tools-and-multi-agents">Quiz 5 - Beyond LLM, Tools and (Multi)-Agents</h2> <h3 id="questions-4">Questions</h3> <p>Question 1: <strong>Which of the following statements are true about emergent abilities in large language models (LLMs)?</strong></p> <ul> <li>A) They are present in both large and small language models</li> <li>B) They appear unexpectedly when the model size surpasses a critical threshold</li> <li>C) They are deliberately engineered into the model during training</li> <li>D) An example of an emergent ability is solving complex arithmetic problems</li> </ul> <p>Question 2: <strong>What are some advantages / limitations of large language models (LLMs)?</strong></p> <ul> <li>A) LLM can generate toxic, biased, or misleading content, posing societal risk</li> <li>B) Training larger models reduces risks of overfitting and data memorization</li> <li>C) Massive datasets required for training can amplify existing biases</li> <li>D) All limitations of LLMs are fully understood and documented</li> </ul> <p>Question 3: <strong>How can the capabilities of LLMs be improved?</strong></p> <ul> <li>A) Reducing the size of the models to prevent overfitting</li> <li>B) Developing new model architectures and using higher-quality data</li> <li>C) Focusing research on making emergent abilities accessible to smaller models</li> <li>D) Limiting training to known data to prevent unknown limitations</li> </ul> <p>Question 4: <strong>Which statements correctly describe LLM agents as per the course summary?</strong></p> <ul> <li>A) An LLM agent uses a large language model as the main controller</li> <li>B) They can perform complex tasks using advanced planning techniques and tools</li> <li>C) The concept of LLM agents was first introduced in early 2021</li> <li>D) LLM agents cannot use external tools like web search and code interpreters</li> </ul> <p>Question 5: <strong>Why are agents preferred over single LLM calls for complex tasks?</strong></p> <ul> <li>A) They can plan sequences of actions and learn from experiences</li> <li>B) Agents are limited to predefined responses and cannot adapt to new tasks</li> <li>C) Agents can perform complex tasks by utilizing external tools</li> <li>D) Parallelisation offers faster answers for Agents based LLM than single LLM</li> </ul> <p>Question 6: <strong>Which of the following are key components of an LLM agent?</strong></p> <ul> <li>A) Agent/Brain: The LLM acting as the main controller and decision-maker</li> <li>B) Randomization Module: Introducing randomness to the agent’s decisions</li> <li>C) Memory: Storing past interactions and experiences</li> <li>D) Emotion Engine: Simulating human emotions in the agent’s responses</li> </ul> <p>Question 7: <strong>What does “reflection” refer to?</strong></p> <ul> <li>A) The agent’s ability to mirror the user’s language style</li> <li>B) A framework allowing agent to improve performance using linguistic feedback</li> <li>C) The process of agents self-evaluating their actions for errors</li> <li>D) Agent mechanism transforming environmental feedback into improvements</li> </ul> <p>Question 8: <strong>What are some current limitations of LLM agents and multi-agent systems?</strong></p> <ul> <li>A) Ensuring agents don’t deviate from the initial plan - divergence of planning</li> <li>B) Guaranteeing that agents can perform tasks without any form of communication</li> <li>C) Developing robust methods for testing and evaluating agent performance</li> <li>D) Trust and security in real-world deployments</li> </ul> <h3 id="solutions-4">Solutions</h3> <ul> <li>Solution 1: B, D</li> <li>Solution 2: A, C</li> <li>Solution 3: B, C</li> <li>Solution 4: A, B</li> <li>Solution 5: C</li> <li>Solution 6: A, C</li> <li>Solution 7: B</li> <li>Solution 8: A, D</li> </ul> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Florian Bastin. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: October 03, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-E6GFW5ZTBH"></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>